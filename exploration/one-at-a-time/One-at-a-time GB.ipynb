{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model per feature\n",
    "The people of Kaggle have hypothesized that the given data are actually the result of the application of PCA to an original dataset because:\n",
    "1. The features seem independent.\n",
    "2. There are exactly 200 features.\n",
    "\n",
    "Thus, it might be a good idea to make a predictor for each feature and then aggregate them; this will prevent us from finding spurious correlations between features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import time\n",
    "import sys\n",
    "sys.path.insert(0, '../utils')\n",
    "# these are our own\n",
    "from data_utils import *\n",
    "from gbm_wrappers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and prepare the data\n",
    "X = pd.read_csv(\"../data/train.csv\")\n",
    "X.pop(\"ID_code\")\n",
    "t = X.pop(\"target\")\n",
    "# split it into train (80%) and val (20%)\n",
    "train_i, val_i, _ = split_data_indices(len(X), w=[0.8, 0.2, 0])\n",
    "X_train = X[train_i]\n",
    "t_train = t[train_i]\n",
    "X_val = X[val_i]\n",
    "t_val = t[val_i]\n",
    "\n",
    "# test data\n",
    "test = pd.read_csv('../data/test.csv')\n",
    "test_id = test.pop(\"ID_code\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now train a GBM for each feature, following [this kernel](https://www.kaggle.com/ymatioun/santander-model-one-feature-at-a-time)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = range(X.shape[1])\n",
    "val_individual_preds = np.zeros(X_val.shape)\n",
    "train_individual_preds = np.zeros(X_train.shape)\n",
    "val_individual_auc = np.zeros(X.shape[1])\n",
    "train_individual_auc = np.zeros(X.shape[1])\n",
    "\n",
    "test_individual_preds = np.zeros(test.shape)\n",
    "\n",
    "# These recommendations as seen in the kernel (thank you Youri Matiounine):\n",
    "# Use lightgbm for prediction\n",
    "# Assume all features are independent, so fit model to one feature at a time\n",
    "# Then final prediction is a product of all predictions based on a single feature\n",
    "# Since data contains only one feature, do not use CV - just used fixed number of iterations\n",
    "params = {\n",
    "    'task': 'train', 'max_depth': 1, 'boosting_type': 'gbdt',\n",
    "    'objective': 'binary', 'num_leaves': 3, 'learning_rate': 0.1,\n",
    "    'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 5,\n",
    "    'lambda_l1': 1, 'lambda_l2': 60, 'verbose': -99\n",
    "}\n",
    "\n",
    "for f in features:\n",
    "    # train a model with just this feature\n",
    "    lgb_train = lgb.Dataset(X_train.iloc[:,f:f+1], t_train)\n",
    "    gbm = lgb.train(params, lgb_train, 45, verbose_eval=1000)\n",
    "    \n",
    "    # store individual validation and training predictions and AUC (not in kernel)\n",
    "    #train\n",
    "    train_individual_preds[:,f] = gbm.predict(X_train.iloc[:,f:f+1], num_iteration=gbm.best_iteration)\n",
    "    fpr, tpr, _ = roc_curve(t_train, train_individual_preds[:,f])\n",
    "    train_individual_auc[f] = auc(fpr, tpr)\n",
    "    # val\n",
    "    val_individual_preds[:,f] = gbm.predict(X_val.iloc[:,f:f+1], num_iteration=gbm.best_iteration)\n",
    "    fpr, tpr, _ = roc_curve(t_val, val_individual_preds[:,f])\n",
    "    val_individual_auc[f] = auc(fpr, tpr)\n",
    "    # test\n",
    "    test_individual_preds[:,f] = gbm.predict(test.iloc[:,f:f+1], num_iteration=gbm.best_iteration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That same kernel then suggests to take a product of individual predictions as a way of aggregating them. Let's see how well that would work on our validation set. Notice how the validation set has had nothing to do with the training so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train AUC=0.9110\n",
      "Val AUC=0.9032\n"
     ]
    }
   ],
   "source": [
    "print(\"Train AUC={:.4f}\".format(auc_wrapper(train_individual_preds, t_train)))\n",
    "print(\"Val AUC={:.4f}\".format(auc_wrapper(val_individual_preds, t_val)))\n",
    "# also predict on test set for submission\n",
    "test_preds = (10 * test_individual_preds).prod(axis=1)\n",
    "submission = pd.DataFrame({'ID_code': test_id, 'target': test_preds.astype('float32')})\n",
    "submission.to_csv('sub1f.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This submission file obtained a test AUC of $0.89885$. Now we deviate from the kernel. Let's start by trying different ways of averaging the observations. In particular I tried weighting each feature by the $AUC$ of its individual predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Casual mean\n",
      "Train AUC=0.9106\n",
      "Val AUC=0.8961\n",
      "\n",
      "Casual mean weights are train individual AUC\n",
      "Train AUC=0.9102\n",
      "Val AUC=0.8960\n",
      "\n",
      "Product mean weights are train individual AUC\n",
      "Train AUC=0.9121\n",
      "Val AUC=0.8972\n",
      "\n",
      "Let go of the worst 10 individual AUC features\n",
      "Train AUC=0.8891\n",
      "Val AUC=0.8718\n"
     ]
    }
   ],
   "source": [
    "print(\"Casual mean\")\n",
    "print(\"Train AUC={:.4f}\".format(auc_wrapper(train_individual_preds, t_train, \"mean\")))\n",
    "print(\"Val AUC={:.4f}\".format(auc_wrapper(val_individual_preds, t_val, \"mean\")))\n",
    "\n",
    "print(\"\\nCasual mean weights are train individual AUC\")\n",
    "print(\"Train AUC={:.4f}\".format(\n",
    "    auc_wrapper(train_individual_preds, t_train, \"weighted\", train_individual_auc)\n",
    "    ))\n",
    "print(\"Val AUC={:.4f}\".format(\n",
    "    auc_wrapper(val_individual_preds, t_val, \"weighted\", train_individual_auc)\n",
    "    ))\n",
    "\n",
    "print(\"\\nProduct mean weights are train individual AUC\")\n",
    "print(\"Train AUC={:.4f}\".format(\n",
    "    auc_wrapper(train_individual_preds, t_train, \"prod_weights\", train_individual_auc)\n",
    "    ))\n",
    "print(\"Val AUC={:.4f}\".format(\n",
    "    auc_wrapper(val_individual_preds, t_val, \"prod_weights\", train_individual_auc)\n",
    "    ))\n",
    "\n",
    "print(\"\\nLet go of the worst 10 individual AUC features\")\n",
    "to_keep = np.argsort(train_individual_auc)[:-10]\n",
    "print(\"Train AUC={:.4f}\".format(\n",
    "    auc_wrapper(train_individual_preds, t_train, \"keep\", None, to_keep)\n",
    "    ))\n",
    "print(\"Val AUC={:.4f}\".format(\n",
    "    auc_wrapper(val_individual_preds, t_val, \"keep\", None, to_keep)\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "None of these was really better then the rest. Losing variables, however, seems like a bad idea. This last observation, along with the small difference between training and validation $AUC$ is an indicator that our model could use higher complexity to further decrease bias. We now try to increase the model complexity by retuning parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = range(X.shape[1])\n",
    "val_individual_preds = np.zeros(X_val.shape)\n",
    "train_individual_preds = np.zeros(X_train.shape)\n",
    "val_individual_auc = np.zeros(X.shape[1])\n",
    "train_individual_auc = np.zeros(X.shape[1])\n",
    "\n",
    "# These recommendations as seen in the kernel (thank you Youri Matiounine):\n",
    "# Use lightgbm for prediction\n",
    "# Assume all features are independent, so fit model to one feature at a time\n",
    "# Then final prediction is a product of all predictions based on a single feature\n",
    "# Since data contains only one feature, do not use CV - just used fixed number of iterations\n",
    "params = {\n",
    "    'task': 'train', 'max_depth': 1, 'boosting_type': 'gbdt',\n",
    "    'objective': 'binary', 'num_leaves': 5, 'learning_rate': 0.1,\n",
    "    'feature_fraction': 0.9, 'bagging_fraction': 0.8, 'bagging_freq': 5,\n",
    "    'lambda_l1': 1, 'lambda_l2': 60, 'verbose': -99\n",
    "}\n",
    "\n",
    "for f in features:\n",
    "    # train a model with just this feature\n",
    "    lgb_train = lgb.Dataset(X_train.iloc[:,f:f+1], t_train)\n",
    "    gbm = lgb.train(params, lgb_train, 45, verbose_eval=1000)\n",
    "    \n",
    "    # store individual validation and training predictions and AUC (not in kernel)\n",
    "    #train\n",
    "    train_individual_preds[:,f] = gbm.predict(X_train.iloc[:,f:f+1], num_iteration=gbm.best_iteration)\n",
    "    fpr, tpr, _ = roc_curve(t_train, train_individual_preds[:,f])\n",
    "    train_individual_auc[f] = auc(fpr, tpr)\n",
    "    # val\n",
    "    val_individual_preds[:,f] = gbm.predict(X_val.iloc[:,f:f+1], num_iteration=gbm.best_iteration)\n",
    "    fpr, tpr, _ = roc_curve(t_val, val_individual_preds[:,f])\n",
    "    val_individual_auc[f] = auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train AUC=0.9115\n",
      "Val AUC=0.9020\n"
     ]
    }
   ],
   "source": [
    "print(\"Train AUC={:.4f}\".format(auc_wrapper(train_individual_preds, t_train)))\n",
    "print(\"Val AUC={:.4f}\".format(auc_wrapper(val_individual_preds, t_val)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
